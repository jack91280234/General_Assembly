{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit Scraping - Project 4\n",
    "----\n",
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scenario\n",
    "\n",
    "You're fresh out of your Data Science bootcamp and looking to break through in the world of freelance data journalism. Nate Silver and co. at FiveThirtyEight have agreed to hear your pitch for a story in two weeks!\n",
    "\n",
    "Your piece is going to be on how to create a Reddit post that will get the most engagement from Reddit users. Because this is FiveThirtyEight, you're going to have to get data and analyze it in order to make a compelling narrative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start by importing some libraries that are neccessary for web scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imported chromedriver, had to redownload the latest chromedriver and insert the file into the project file path because the current chrome browser wouldn't work with the earlier version of chromedriver.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(executable_path=\"./chromedriver\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting chromedriver to retrieve html from reddit, not sure what the timer is for exactly need to get info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"http://www.reddit.com\")\n",
    "\n",
    "# Give it a second to load the page\n",
    "\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grabbing the page content and setting it to a html variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "html = driver.page_source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use BeautifulSoup parser for our html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html, 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Close our chrome driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List comprehension for specifc reddit tags. Used xpath and beautifulsoup to search where important variables for our models are located. Removed the first element in some lists that were made because reddit's first post are advertisements that arent helpful for our model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# List comprehension for reddit titles\n",
    "\n",
    "rtitle = [x.text for x in soup.find_all('a', {'data-event-action':'title'})]\n",
    "rtitle.pop(0)\n",
    "\n",
    "# List comprehension for subreddits of reddit posts\n",
    "\n",
    "subreddit = [x.text for x in soup.find_all('a', {'class':'subreddit hover may-blank'})]\n",
    "\n",
    "# List comprehension for reddit post timestamps\n",
    "\n",
    "rtime = [x.text for x in soup.find_all('time', {'class':'live-timestamp'})]\n",
    "\n",
    "# List comprehension for reddit total comments of post\n",
    "\n",
    "comments = [x.text for x in soup.find_all('a', {'class':'bylink comments may-blank'})]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tried to make a reddit web scraping function but failed. \n",
    "\n",
    "A large portion of the project time was spent building this reddit scraper. The scraper could pull posts off the page but the posts pulled were 90% duplicates (~18,000 to ~500 non-duplicates).\n",
    "\n",
    "\n",
    "    def reddit_scraper(website):\n",
    "    \n",
    "    driver = webdriver.Chrome(executable_path=\"./chromedriver\")\n",
    "    driver.get(website)\n",
    "    \n",
    "    time.sleep(1)\n",
    "    \n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "        \n",
    "    titles = [x. text for x in soup.find_all('a', {'data-event-action':'title'})]\n",
    "    titles.pop(0)    \n",
    "    \n",
    "    upvotes = [x.text for x in soup.find_all('div', {'class':'score unvoted'})]\n",
    "    upvotes.pop(0)\n",
    "    \n",
    "    subreddit = [x.text for x in soup.find_all('a', {'class':'subreddit hover may-blank'})]\n",
    "    \n",
    "    rtime = [x.text for x in soup.find_all('time', {'class':'live-timestamp'})]\n",
    "    \n",
    "    comments = [x.text for x in soup.find_all('a', {'class':'bylink comments may-blank'})]\n",
    "    \n",
    "    dataframe = pd.DataFrame(\n",
    "    {'titles': titles,\n",
    "     'upvotes': upvotes,\n",
    "     'subreddit': subreddit,\n",
    "     'time_stamp': rtime,\n",
    "     'num_comments': comments\n",
    "    })\n",
    "    \n",
    "    ids=[]\n",
    "        \n",
    "    for x in soup.findAll('div', {'class': 'thing'}):\n",
    "        ids.append(x['data-fullname'])\n",
    "            \n",
    "    max_results = 20001\n",
    "    \n",
    "    for i in range(25, max_results, 25):\n",
    "        \n",
    "        url_template = \"http://www.reddit.com/?count={}&after={}\".format(i, ids[-1])\n",
    "        driver.get(url_template)\n",
    "        \n",
    "        time.sleep(1)\n",
    "        \n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        \n",
    "        # Additional titles added onto titles varaible \n",
    "        \n",
    "        titles_2 = []\n",
    "        \n",
    "        for x in soup.find_all('a', {'data-event-action':'title'}):\n",
    "            titles_2.append(x.text)\n",
    "            \n",
    "        titles_2.pop(0)\n",
    "        \n",
    "        # Additional upvote numbers added onto upvote varaible \n",
    "        \n",
    "        upvotes_2 = []\n",
    "        \n",
    "        for x in soup.find_all('div', {'class':'score unvoted'}):\n",
    "            upvotes_2.append(x.text)\n",
    "            \n",
    "        upvotes_2.pop(0)\n",
    "        \n",
    "        # Additional subreddit groups added onto subreddit varaible \n",
    "        \n",
    "        subreddit_2 = []\n",
    "        \n",
    "        for x in soup.find_all('a', {'class':'subreddit hover may-blank'}):\n",
    "            subreddit_2.append(x.text)\n",
    "        \n",
    "        # Additional time stamps added onto rtime varaible \n",
    "        \n",
    "        rtime_2 = []\n",
    "        \n",
    "        for x in soup.find_all('time', {'class':'live-timestamp'}):\n",
    "            rtime_2.append(x.text)\n",
    "                                     \n",
    "        # Additional comment numbers added onto comments varaible \n",
    "        \n",
    "        comments_2 = []\n",
    "        \n",
    "        for x in soup.find_all('a', {'class':'bylink comments may-blank'}):\n",
    "            comments_2.append(x.text)\n",
    "  \n",
    "        # Adding additional ids to id's variable allowing us to see the next page id\n",
    "        \n",
    "        for x in soup.findAll('div', {'class': 'thing'}):\n",
    "            ids.append(x['data-fullname'])\n",
    "    \n",
    "        try:\n",
    "            dataframe_1 = pd.DataFrame({'titles': titles_2,\n",
    "                                  'upvotes': upvotes_2,\n",
    "                                  'subreddit': subreddit_2,\n",
    "                                  'time_stamp': rtime_2,\n",
    "                                  'num_comments': comments_2\n",
    "                                 })\n",
    "            dataframe = pd.concat([dataframe, dataframe_1])\n",
    "            \n",
    "        except:\n",
    "            pass       \n",
    "               \n",
    "        time.sleep(1)\n",
    "    \n",
    "    dataframe.reset_index(inplace = True)\n",
    "    driver.close()\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Sophie's dataset to build my models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./scraping_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### May add additional code to try and webscrape myself below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code for later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examining the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>created_at</th>\n",
       "      <th>id</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>time_delta</th>\n",
       "      <th>time_now</th>\n",
       "      <th>title</th>\n",
       "      <th>upvotes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-22 16:36:35</td>\n",
       "      <td>470616764</td>\n",
       "      <td>1153</td>\n",
       "      <td>gifs</td>\n",
       "      <td>0 days 04:24:57.883078000</td>\n",
       "      <td>2018-01-22 21:01:32.883070</td>\n",
       "      <td>Finnish ski jumping team</td>\n",
       "      <td>86005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-01-22 16:37:28</td>\n",
       "      <td>470617063</td>\n",
       "      <td>238</td>\n",
       "      <td>pics</td>\n",
       "      <td>0 days 04:24:04.883093000</td>\n",
       "      <td>2018-01-22 21:01:32.883092</td>\n",
       "      <td>Super excited about motherhood</td>\n",
       "      <td>20336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2018-01-22 16:50:12</td>\n",
       "      <td>470621440</td>\n",
       "      <td>844</td>\n",
       "      <td>funny</td>\n",
       "      <td>0 days 04:11:20.883101000</td>\n",
       "      <td>2018-01-22 21:01:32.883100</td>\n",
       "      <td>Messing with the new guy.</td>\n",
       "      <td>17611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2018-01-22 17:25:20</td>\n",
       "      <td>470633617</td>\n",
       "      <td>380</td>\n",
       "      <td>space</td>\n",
       "      <td>0 days 03:36:12.883109000</td>\n",
       "      <td>2018-01-22 21:01:32.883108</td>\n",
       "      <td>NASA cancels and postpones all of their public...</td>\n",
       "      <td>11178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2018-01-22 15:41:56</td>\n",
       "      <td>470598246</td>\n",
       "      <td>430</td>\n",
       "      <td>technology</td>\n",
       "      <td>0 days 05:19:36.883120000</td>\n",
       "      <td>2018-01-22 21:01:32.883118</td>\n",
       "      <td>New Bill Would Stop States From Banning Broadb...</td>\n",
       "      <td>13467</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0           created_at         id  num_comments   subreddit  \\\n",
       "0           0  2018-01-22 16:36:35  470616764          1153        gifs   \n",
       "1           1  2018-01-22 16:37:28  470617063           238        pics   \n",
       "2           2  2018-01-22 16:50:12  470621440           844       funny   \n",
       "3           3  2018-01-22 17:25:20  470633617           380       space   \n",
       "4           4  2018-01-22 15:41:56  470598246           430  technology   \n",
       "\n",
       "                  time_delta                    time_now  \\\n",
       "0  0 days 04:24:57.883078000  2018-01-22 21:01:32.883070   \n",
       "1  0 days 04:24:04.883093000  2018-01-22 21:01:32.883092   \n",
       "2  0 days 04:11:20.883101000  2018-01-22 21:01:32.883100   \n",
       "3  0 days 03:36:12.883109000  2018-01-22 21:01:32.883108   \n",
       "4  0 days 05:19:36.883120000  2018-01-22 21:01:32.883118   \n",
       "\n",
       "                                               title  upvotes  \n",
       "0                           Finnish ski jumping team    86005  \n",
       "1                     Super excited about motherhood    20336  \n",
       "2                          Messing with the new guy.    17611  \n",
       "3  NASA cancels and postpones all of their public...    11178  \n",
       "4  New Bill Would Stop States From Banning Broadb...    13467  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5567, 9)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0       int64\n",
       "created_at      object\n",
       "id               int64\n",
       "num_comments     int64\n",
       "subreddit       object\n",
       "time_delta      object\n",
       "time_now        object\n",
       "title           object\n",
       "upvotes          int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'Unnamed: 0', u'created_at', u'id', u'num_comments', u'subreddit',\n",
       "       u'time_delta', u'time_now', u'title', u'upvotes'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping the \"Unnamed: 0\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('Unnamed: 0', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing duplicate titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates('title', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5405, 8)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert all number of comments 15 and below to 0 and all number of comments above 15 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df.drop('num_comments', axis = 1)\n",
    "y = df.num_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = y.apply(lambda x: 0 if x <= 15 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>time_delta</th>\n",
       "      <th>time_now</th>\n",
       "      <th>title</th>\n",
       "      <th>upvotes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-22 16:36:35</td>\n",
       "      <td>470616764</td>\n",
       "      <td>gifs</td>\n",
       "      <td>0 days 04:24:57.883078000</td>\n",
       "      <td>2018-01-22 21:01:32.883070</td>\n",
       "      <td>Finnish ski jumping team</td>\n",
       "      <td>86005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-22 16:37:28</td>\n",
       "      <td>470617063</td>\n",
       "      <td>pics</td>\n",
       "      <td>0 days 04:24:04.883093000</td>\n",
       "      <td>2018-01-22 21:01:32.883092</td>\n",
       "      <td>Super excited about motherhood</td>\n",
       "      <td>20336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-22 16:50:12</td>\n",
       "      <td>470621440</td>\n",
       "      <td>funny</td>\n",
       "      <td>0 days 04:11:20.883101000</td>\n",
       "      <td>2018-01-22 21:01:32.883100</td>\n",
       "      <td>Messing with the new guy.</td>\n",
       "      <td>17611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-22 17:25:20</td>\n",
       "      <td>470633617</td>\n",
       "      <td>space</td>\n",
       "      <td>0 days 03:36:12.883109000</td>\n",
       "      <td>2018-01-22 21:01:32.883108</td>\n",
       "      <td>NASA cancels and postpones all of their public...</td>\n",
       "      <td>11178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-22 15:41:56</td>\n",
       "      <td>470598246</td>\n",
       "      <td>technology</td>\n",
       "      <td>0 days 05:19:36.883120000</td>\n",
       "      <td>2018-01-22 21:01:32.883118</td>\n",
       "      <td>New Bill Would Stop States From Banning Broadb...</td>\n",
       "      <td>13467</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            created_at         id   subreddit                 time_delta  \\\n",
       "0  2018-01-22 16:36:35  470616764        gifs  0 days 04:24:57.883078000   \n",
       "1  2018-01-22 16:37:28  470617063        pics  0 days 04:24:04.883093000   \n",
       "2  2018-01-22 16:50:12  470621440       funny  0 days 04:11:20.883101000   \n",
       "3  2018-01-22 17:25:20  470633617       space  0 days 03:36:12.883109000   \n",
       "4  2018-01-22 15:41:56  470598246  technology  0 days 05:19:36.883120000   \n",
       "\n",
       "                     time_now  \\\n",
       "0  2018-01-22 21:01:32.883070   \n",
       "1  2018-01-22 21:01:32.883092   \n",
       "2  2018-01-22 21:01:32.883100   \n",
       "3  2018-01-22 21:01:32.883108   \n",
       "4  2018-01-22 21:01:32.883118   \n",
       "\n",
       "                                               title  upvotes  \n",
       "0                           Finnish ski jumping team    86005  \n",
       "1                     Super excited about motherhood    20336  \n",
       "2                          Messing with the new guy.    17611  \n",
       "3  NASA cancels and postpones all of their public...    11178  \n",
       "4  New Bill Would Stop States From Banning Broadb...    13467  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    1\n",
       "3    1\n",
       "4    1\n",
       "Name: num_comments, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline model accuracy for number of comments will be if we guessed the most frequent option, in this case it is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2755\n",
       "1    2650\n",
       "Name: num_comments, dtype: int64"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline is ~51% for 15 or less comments on a single post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.50971322849213696"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts().values[0]/float(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits = 10, random_state = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reddit Titles\n",
    "#### Count Vectorizer for feature extracting the titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cvec = CountVectorizer(ngram_range = (1,4), lowercase = True, \n",
    "                       stop_words = 'english', max_features = 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit and transforming our train titles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dtm = pd.DataFrame(cvec.fit_transform(X_train.title).todense(), columns=cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Only transforming our train titles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_dtm = pd.DataFrame(cvec.transform(X_test.title).todense(), columns=cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>000 year</th>\n",
       "      <th>01</th>\n",
       "      <th>10</th>\n",
       "      <th>10 year</th>\n",
       "      <th>10 years</th>\n",
       "      <th>10 years ago</th>\n",
       "      <th>10 years ago today</th>\n",
       "      <th>100</th>\n",
       "      <th>...</th>\n",
       "      <th>youtube channel</th>\n",
       "      <th>youtube video</th>\n",
       "      <th>ypg</th>\n",
       "      <th>yuri</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zelda</th>\n",
       "      <th>zero</th>\n",
       "      <th>zone</th>\n",
       "      <th>zu</th>\n",
       "      <th>zum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 10000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  000  000 year  01  10  10 year  10 years  10 years ago  \\\n",
       "0   0    0         0   0   0        0         0             0   \n",
       "1   0    0         0   0   0        0         0             0   \n",
       "2   0    0         0   0   0        0         0             0   \n",
       "3   0    0         0   0   0        0         0             0   \n",
       "4   0    0         0   0   0        0         0             0   \n",
       "\n",
       "   10 years ago today  100 ...   youtube channel  youtube video  ypg  yuri  \\\n",
       "0                   0    0 ...                 0              0    0     0   \n",
       "1                   0    0 ...                 0              0    0     0   \n",
       "2                   0    0 ...                 0              0    0     0   \n",
       "3                   0    0 ...                 0              0    0     0   \n",
       "4                   0    0 ...                 0              0    0     0   \n",
       "\n",
       "   zealand  zelda  zero  zone  zu  zum  \n",
       "0        0      0     0     0   0    0  \n",
       "1        0      0     0     0   0    0  \n",
       "2        0      0     0     0   0    0  \n",
       "3        0      0     0     0   0    0  \n",
       "4        0      0     0     0   0    0  \n",
       "\n",
       "[5 rows x 10000 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_dtm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counting the most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function sklearn.feature_extraction.text.<lambda>>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvec.build_analyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'new', 149),\n",
       " (u'just', 142),\n",
       " (u'time', 115),\n",
       " (u'like', 101),\n",
       " (u'game', 76),\n",
       " (u'people', 73),\n",
       " (u'oc', 72),\n",
       " (u'today', 71),\n",
       " (u'got', 71),\n",
       " (u'year', 69)]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries = \"\".join(df.title)\n",
    "ngrams_summaries = cvec.build_analyzer()(summaries)\n",
    "\n",
    "Counter(ngrams_summaries).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multinomial Naive Base Model Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnnb = MultinomialNB()\n",
    "mnnb.fit(X_train_dtm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.56950672645739908"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnnb.score(X_test_dtm, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression Classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = cross_val_score(lr, X_test_dtm, y_test, cv = cv, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.524054115317 std: +/- 0.035165393884\n"
     ]
    }
   ],
   "source": [
    "print 'mean: {} std: +/- {}'.format(np.mean(score), np.std(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making a pipeline for our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline for titles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cv_title_model(vectorizer, model):\n",
    "    my_pipe = Pipeline([('vectorizer', vectorizer),\n",
    "                        ('model', model)])\n",
    "    results = cross_val_score(my_pipe, X_train.title, y_train, cv = cv, n_jobs = -1, verbose = 1)\n",
    "    my_pipe.fit(X_train.title, y_train)\n",
    "    \n",
    "    print \"Number of features: \", len(my_pipe.named_steps['vectorizer'].get_feature_names())\n",
    "    print \"Train Set Accuracy {:.4f}\".format(my_pipe.score(X_train.title, y_train))\n",
    "    print \"Stratified KFold Cross Val Accuracy {:.4f} +/- {:.4f}\".format(results.mean(), results.std())\n",
    "    print \"TestSet Accuracy: {:.4f}\".format(my_pipe.score(X_test.title, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple tests on different count vectorizers using Multinomial Naive Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cvec_1 = CountVectorizer(ngram_range = (1,2), lowercase = True, stop_words = 'english')\n",
    "cvec_2 = CountVectorizer(ngram_range = (2,4), lowercase = True, stop_words = 'english')\n",
    "cvec_3 = CountVectorizer(ngram_range = (3,4), lowercase = True, stop_words = 'english', max_features = 5000)\n",
    "cvec_4 = CountVectorizer(ngram_range = (4,5), lowercase = True, stop_words = 'english', max_features = 5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'new', 149),\n",
       " (u'just', 142),\n",
       " (u'time', 115),\n",
       " (u'like', 101),\n",
       " (u'game', 76),\n",
       " (u'people', 73),\n",
       " (u'oc', 72),\n",
       " (u'today', 71),\n",
       " (u'got', 71),\n",
       " (u'year', 69)]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cvec_1 top 10 words\n",
    "summaries = \"\".join(df.title)\n",
    "ngrams_summaries = cvec_1.build_analyzer()(summaries)\n",
    "Counter(ngrams_summaries).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'super bowl', 20),\n",
       " (u'years ago', 17),\n",
       " (u'year old', 16),\n",
       " (u'looks like', 13),\n",
       " (u'alexis sanchez', 12),\n",
       " (u'fuck sony', 10),\n",
       " (u'manchester united', 9),\n",
       " (u'government shutdown', 8),\n",
       " (u'fuck sony fuck', 8),\n",
       " (u'sony fuck sony', 8)]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cvec_2 top 10 words\n",
    "ngrams_summaries = cvec_2.build_analyzer()(summaries)\n",
    "Counter(ngrams_summaries).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'fuck sony fuck', 8),\n",
       " (u'sony fuck sony', 8),\n",
       " (u'fuck sony fuck sony', 8),\n",
       " (u'sony fuck sony fuck', 7),\n",
       " (u'years ago today', 6),\n",
       " (u'thought guys like', 3),\n",
       " (u'world richest 82', 3),\n",
       " (u'world richest 82 wealth', 3),\n",
       " (u'tripadvisor fake restaurant', 3),\n",
       " (u'nfc championship game', 3)]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cvec_3 top 10 words\n",
    "ngrams_summaries = cvec_3.build_analyzer()(summaries)\n",
    "Counter(ngrams_summaries).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'fuck sony fuck sony', 8),\n",
       " (u'fuck sony fuck sony fuck', 7),\n",
       " (u'sony fuck sony fuck sony', 7),\n",
       " (u'sony fuck sony fuck', 7),\n",
       " (u'world richest 82 wealth', 3),\n",
       " (u'escorted marshals attend white school', 2),\n",
       " (u'orange county ready clear', 2),\n",
       " (u'white school 1960 african', 2),\n",
       " (u'new orleans school desegregation crisis', 2),\n",
       " (u'accent imitate india prime', 2)]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cvec_4 top 10 words\n",
    "ngrams_summaries = cvec_4.build_analyzer()(summaries)\n",
    "Counter(ngrams_summaries).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:    0.5s remaining:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features:  25757\n",
      "Train Set Accuracy 0.9751\n",
      "Stratified KFold Cross Val Accuracy 0.5432 +/- 0.0266\n",
      "TestSet Accuracy: 0.5740\n"
     ]
    }
   ],
   "source": [
    "# cvec_1 multinomial model score\n",
    "cv_title_model(cvec_1, mnnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:    0.7s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features:  41550\n",
      "Train Set Accuracy 0.9655\n",
      "Stratified KFold Cross Val Accuracy 0.5344 +/- 0.0156\n",
      "TestSet Accuracy: 0.5320\n"
     ]
    }
   ],
   "source": [
    "# cvec_2 multinomial model score\n",
    "cv_title_model(cvec_2, mnnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:    0.5s remaining:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features:  5000\n",
      "Train Set Accuracy 0.7385\n",
      "Stratified KFold Cross Val Accuracy 0.5106 +/- 0.0051\n",
      "TestSet Accuracy: 0.5135\n"
     ]
    }
   ],
   "source": [
    "# cvec_3 multinomial model score\n",
    "cv_title_model(cvec_3, mnnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:    0.5s remaining:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features:  5000\n",
      "Train Set Accuracy 0.7213\n",
      "Stratified KFold Cross Val Accuracy 0.5112 +/- 0.0036\n",
      "TestSet Accuracy: 0.5112\n"
     ]
    }
   ],
   "source": [
    "# cvec_4 multinomial model score\n",
    "cv_title_model(cvec_4, mnnb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple tests on different count vectorizers using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:    0.6s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features:  25757\n",
      "Train Set Accuracy 0.9878\n",
      "Stratified KFold Cross Val Accuracy 0.5587 +/- 0.0215\n",
      "TestSet Accuracy: 0.5824\n"
     ]
    }
   ],
   "source": [
    "# cvec_1 logistic regression model score\n",
    "cv_title_model(cvec_1, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:    0.9s remaining:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    1.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features:  41550\n",
      "Train Set Accuracy 0.9680\n",
      "Stratified KFold Cross Val Accuracy 0.5187 +/- 0.0126\n",
      "TestSet Accuracy: 0.5213\n"
     ]
    }
   ],
   "source": [
    "# cvec_2 logistic regression model score\n",
    "cv_title_model(cvec_2, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:    0.5s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features:  5000\n",
      "Train Set Accuracy 0.7399\n",
      "Stratified KFold Cross Val Accuracy 0.5104 +/- 0.0049\n",
      "TestSet Accuracy: 0.5129\n"
     ]
    }
   ],
   "source": [
    "# cvec_3 logistic regression model score\n",
    "cv_title_model(cvec_3, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:    0.4s remaining:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features:  5000\n",
      "Train Set Accuracy 0.7219\n",
      "Stratified KFold Cross Val Accuracy 0.5112 +/- 0.0036\n",
      "TestSet Accuracy: 0.5112\n"
     ]
    }
   ],
   "source": [
    "# cvec_4 logistic regression model score\n",
    "cv_title_model(cvec_4, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reddit Subreddits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dummy subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xs = pd.get_dummies(df.subreddit, drop_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DDLC                    90\n",
       "aww                     87\n",
       "funny                   84\n",
       "eagles                  76\n",
       "reddevils               75\n",
       "todayilearned           50\n",
       "Patriots                48\n",
       "PewdiepieSubmissions    47\n",
       "PrequelMemes            43\n",
       "memes                   42\n",
       "dtype: int64"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xs.sum().sort_values(ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(class_weight='balanced', n_jobs=-1, n_estimators=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:   23.5s finished\n"
     ]
    }
   ],
   "source": [
    "score = cross_val_score(rf, Xs, y, cv = cv, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.64071335660984452"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    1.6s finished\n"
     ]
    }
   ],
   "source": [
    "score = cross_val_score(lr, Xs, y, cv = cv, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.63979324981173413"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multinomial NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    1.1s finished\n"
     ]
    }
   ],
   "source": [
    "score = cross_val_score(mnnb, Xs, y, cv = cv, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.63979324981173413"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Findings and Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Baseline for our model is __~51%__\n",
    "\n",
    "__Models on reddit titles__\n",
    "\n",
    "* Multinomial NB Model using count vectorizer on reddit titles best score found was __~57%__\n",
    "* Logistic Regression using count vectorizer on reddit titles best score found was __~58%__\n",
    "\n",
    "__Most common words from best model__\n",
    "\n",
    "1. 'new', 149 times\n",
    "2. 'just', 142 times\n",
    "3. 'time', 115 times\n",
    "4. 'like', 101 times\n",
    "5. 'game', 76 times\n",
    "6. 'people', 73 times\n",
    "7. 'oc', 72 times\n",
    "8. 'today', 71 times\n",
    "9. 'got', 71 times\n",
    "10. 'year', 69 times\n",
    "\n",
    "__Models on reddit subreddits__\n",
    "\n",
    "* Random Forest Classifier on dummied reddit subreddit best score found was __~64%__\n",
    "* Logistic Regression on dummied reddit subreddit best score found was __~64%__\n",
    "* Multinomial NB on dummied reddit subreddit best score found was __~64%__\n",
    "\n",
    "__Most common subreddits__\n",
    "\n",
    "1. DDLC, 90 times\n",
    "2. aww, 87 times\n",
    "3. funny, 84 times\n",
    "4. eagles, 76 times\n",
    "5. reddevils, 75 times\n",
    "6. todayilearned, 50 times\n",
    "7. Patriots, 48 times\n",
    "8. PewdiepieSubmissions, 47 times\n",
    "9. PrequelMemes, 43 times\n",
    "10. memes, 42 times\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executive Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro to Reddit\n",
    "\n",
    "Ranking at the top 10 visited websites (Alexa Rankings), Reddit is an unbelievable source of enterainment, news, and information scaling across all industries. Users can post information through videos and images that can then be viewed and commented on by others. With 1.6 billions visiters a month, reddit is a powerhouse when it comes to attention. Through sheer user volumn alone, reddit posts can easily garner millions of views. But with a high user base coupled with posters always seeking the most attention, what can we do to make our post views higher than everyone else?\n",
    "\n",
    "### Purpose and Scope of Work\n",
    "\n",
    "To tackle the problem getting our posts views above the rest, we first need to gather data about past reddit posts. Luckily, reddit allows users to scrape their website as long as their criteria is upheld. The particular important pieces of information that we are try to scrape are the titles of posts, the subreddit of posts, the number of comments, the number of upvotes, and the date of posts. Getting the most information is going to be helpful for building our models in the future but with additional information comes additional EDA. Additional EDA was done to change integer values in columns from the string form to integer. \n",
    "\n",
    "The primary focus for this particular project was to classify our reddit posts based on the number of comments on each post. To simply our target which was our number of posts, I took the median of the comments which came out to be 15 and replaced any comments 15 and below with the value __0__ and number of comments above 15 were replaced with __1__. Through this classfication, we now have a basic \"model\" based on probability with a baseline of __~51%__. The two major features used to make our predictive models were reddit titles and the subreddits. I began my models using reddit titles.\n",
    "\n",
    "Using sklearn's count vectorizer, I turned reddit titles to new features for our model. Using __Multinomial Naive Base__ and __Logistic Regression__ as my predictive models, the best scores from these models were __~57%__ and __~58%__ respectively. With a slight improvement from our baseline model, titles with words such as __'new', 'time', 'game' and 'oc'__ were some of the most predictive words.\n",
    "\n",
    "Using subreddits instead, I could dummy the data to do our feature extraction. Using __Random Forrest Classifier__, __Logistic Regression__ and __Multinomial Naive Base__ as my predictive models, the best scores from these models were __~64%__, __~64%__ and __~64%__ respectively. The models were slightly better then the models using titles as features with subreddits __'DDLC', 'aww', 'funny', and 'eagles'__ being the most predictive subreddits. \n",
    "\n",
    "### Takeaway\n",
    "\n",
    "Using both the title and subreddit features in our models, we were able to slightly beat our baseline model. We were able to find the most predictive words and subreddits __but they only tell us its predictive power__. It doesnt tell us what words/ subreddits determine higher comment thresholds. For this, we would need to look into the top words and view the effects on our target (aka number of comments). After viewing which words predict higher comments, we can suggest those words in the titles of posts to possibly improve comment count and post in certain subreddits to improve comment count. Additionally, we can webscrape post content for additional features, make models on the upvotes, and make models on the date/time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
